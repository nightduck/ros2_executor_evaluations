{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "ROS_DISTRO = 'rolling'\n",
    "sys.path.insert(0, f'/opt/ros/{ROS_DISTRO}/lib/python3.121/site-packages')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from tracetools_analysis.loading import load_file\n",
    "from tracetools_analysis.processor import Processor\n",
    "from tracetools_analysis.processor.cpu_time import CpuTimeHandler\n",
    "from tracetools_analysis.processor.ros2 import Ros2Handler\n",
    "from tracetools_analysis.utils.cpu_time import CpuTimeDataModelUtil\n",
    "from tracetools_analysis.utils.ros2 import Ros2DataModelUtil\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "timer_periods = {\"SensorA\" : timedelta(milliseconds=25), \"SensorB\" : timedelta(milliseconds=40), \"SensorC\" : timedelta(milliseconds=50)}\n",
    "\n",
    "def load_dropped_jobs(filename):\n",
    "  with open(filename, 'r') as f:\n",
    "    dropped_jobs = {}\n",
    "    process_line = False\n",
    "    for line in f:\n",
    "      if line.startswith(\"Dropped jobs:\"):\n",
    "        process_line = True\n",
    "        continue\n",
    "      if process_line:\n",
    "        name, stats = line.strip().split(\":\")\n",
    "        dropped, total = [int(x) for x in stats.split(\"/\")]\n",
    "        dropped_jobs[name.strip()] = float(dropped) / float(total) if total > 0 else 0\n",
    "    assert(process_line)\n",
    "    return dropped_jobs\n",
    "def load_inputs(string):\n",
    "  return (load_file(directory_prefix + string), load_dropped_jobs(directory_prefix + string + '.log'))\n",
    "\n",
    "def get_node_name(owner_info):\n",
    "  node_name = owner_info.split(\",\")[0].split(\":\")[1].strip()\n",
    "  return node_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load trace directory or converted trace file\n",
    "directory_prefix = \"data/\"\n",
    "events_dict = {}\n",
    "\n",
    "def load_inputs(string):\n",
    "  return (load_file(directory_prefix + string), load_dropped_jobs(directory_prefix + string + '.log'))\n",
    "\n",
    "trace_names = ['trace-many-to-many.rm.60', 'trace-many-to-many.edf.60',\n",
    "               'trace-many-to-many.events.60', 'trace-many-to-many.default.60',\n",
    "               'trace-many-to-many.rm.80', 'trace-many-to-many.edf.80',\n",
    "               'trace-many-to-many.events.80', 'trace-many-to-many.default.80',\n",
    "               'trace-many-to-many.rm.90', 'trace-many-to-many.edf.90',\n",
    "               'trace-many-to-many.events.90', 'trace-many-to-many.default.90']\n",
    "display_names = ['RM,60%', 'EDF,60%', 'Events,60%', 'Default,60%',\n",
    "                 'RM,80%', 'EDF,80%', 'Events,80%', 'Default,80%',\n",
    "                 'RM,90%', 'EDF,90%', 'Events,90%', 'Default,90%']\n",
    "\n",
    "# trace_names = ['sequences.rm.ro.uu', 'sequences.rm.re.uu',\n",
    "#                'sequences.events.ro.uu', 'sequences.events.re.uu',\n",
    "#                'sequences.default.uu',\n",
    "#                'sequences.rm.ro.hu', 'sequences.rm.re.hu',\n",
    "#                'sequences.events.ro.hu', 'sequences.events.re.hu',\n",
    "#                'sequences.default.hu']\n",
    "# display_names = ['RM (RO),60%', 'RM (RE),60%', 'Events (RO),60%', 'Events (RE),60%', 'Default,60%',\n",
    "#                  'RM (RO),100%', 'RM (RE),100%', 'Events (RO),100%', 'Events (RE),100%', 'Default,100%']\n",
    "# events_dict = {}\n",
    "# for trace, display in zip(trace_names, display_names):\n",
    "#   events_dict[display] = load_inputs(trace)\n",
    "\n",
    "# events_dict[\"RM, (RO)\"] =     (load_file(directory_prefix + 'timers-only-rm-ro'),     load_dropped_jobs(directory_prefix + 'timers-only-rm-ro.log'))\n",
    "# events_dict[\"RM, (RE)\"] =     (load_file(directory_prefix + 'timers-only-rm-re'),     load_dropped_jobs(directory_prefix + 'timers-only-rm-re.log'))\n",
    "# events_dict[\"EDF, (RO)\"] =    (load_file(directory_prefix + 'timers-only-edf-ro'),    load_dropped_jobs(directory_prefix + 'timers-only-edf-ro.log'))\n",
    "# events_dict[\"EDF, (RE)\"] =    (load_file(directory_prefix + 'timers-only-edf-re'),    load_dropped_jobs(directory_prefix + 'timers-only-edf-re.log'))\n",
    "# events_dict[\"Events, (RO)\"] = (load_file(directory_prefix + 'timers-only-events-ro'), load_dropped_jobs(directory_prefix + 'timers-only-events-ro.log'))\n",
    "# events_dict[\"Events, (RE)\"] = (load_file(directory_prefix + 'timers-only-events-re'), load_dropped_jobs(directory_prefix + 'timers-only-events-re.log'))\n",
    "# events_dict[\"Default\"] =      (load_file(directory_prefix + 'timers-only-default'),   load_dropped_jobs(directory_prefix + 'timers-only-default.log'))\n",
    "\n",
    "dropped_df = pd.DataFrame({\"Executor\": [], \"Utilization\": [], \"Node\": [], \"Drop Rate\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_df = None\n",
    "input_data = None\n",
    "wcet_dict = {}\n",
    "\n",
    "# TODO: Debug this, the dropped_jobs variable is empty\n",
    "for trace, display in zip(trace_names, display_names):\n",
    "  events = load_inputs(trace)\n",
    "  utilization = display.split(\",\")[1]\n",
    "  name = display.split(\",\")[0]\n",
    "  dropped_jobs = events[1]\n",
    "  if len(dropped_jobs) == 0:\n",
    "    print(\"No dropped jobs found for \" + name)\n",
    "    continue\n",
    "  for node, drop_rate in dropped_jobs.items():\n",
    "    # temp_df = pd.DataFrame([[name, utilization, node, drop_rate]])\n",
    "    dropped_df = pd.concat([pd.DataFrame([[name, utilization, node, drop_rate]], columns=dropped_df.columns), dropped_df], ignore_index=True)\n",
    "\n",
    "  # # Timer manager processing\n",
    "  # tid = 2750\n",
    "  # trace_events = pd.DataFrame.from_dict(events[0])[['_name', 'timestamp', 'next_tid', 'prev_tid']]\n",
    "  # sched_events = trace_events[(trace_events['next_tid']==tid) | (trace_events['prev_tid']==tid)]\n",
    "\n",
    "  # Process\n",
    "  if len(events[0]) == 0:\n",
    "    print(\"No events found for \" + name)\n",
    "    continue\n",
    "  handler = Ros2Handler.process(events[0])\n",
    "\n",
    "  # Use data model utils to extract information\n",
    "  data_util = Ros2DataModelUtil(handler.data)\n",
    "  callback_symbols = data_util.get_callback_symbols()\n",
    "\n",
    "  # if utilization != \"90%\":\n",
    "  #   continue\n",
    "  \n",
    "  # callback_symbols = ros2_util.get_callback_symbols()\n",
    "  for callback_object in callback_symbols.keys():\n",
    "    owner_info = data_util.get_callback_owner_info(callback_object)\n",
    "    if \"parameter_events\" in owner_info:\n",
    "      continue\n",
    "    owner_name = get_node_name(owner_info)\n",
    "    temp_df = data_util.get_callback_durations(callback_object)\n",
    "    temp_df[\"Executor\"] = name\n",
    "    temp_df[\"Node\"] = owner_name\n",
    "    temp_df[\"Utilization\"] = utilization\n",
    "    if callback_df is None:\n",
    "      callback_df = temp_df\n",
    "    else:\n",
    "      callback_df = pd.concat([callback_df, temp_df], ignore_index=True)\n",
    "\n",
    "    callback_durations = data_util.get_callback_durations(callback_object)[[\"duration\"]].to_numpy(dtype=np.float64)[:-1] / 1000000.0\n",
    "    \n",
    "    if (owner_name + name) not in wcet_dict:\n",
    "      wcet_dict[owner_name + name] = callback_durations.flatten()\n",
    "    else:\n",
    "      wcet_dict[owner_name + name] = np.concatenate((wcet_dict[owner_name + name], callback_durations.flatten())).flatten()\n",
    "\n",
    "    # print(time_per_thread)\n",
    "    # print(owner_info)\n",
    "    # print(callback_durations)\n",
    "\n",
    "total_drops_df = dropped_df[dropped_df[\"Node\"] == \"Total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "ax = sns.barplot(total_drops_df, y=\"Drop Rate\", x=\"Executor\", hue=\"Utilization\", palette=[\"#003f5c\", \"#7393B3\", \"#7a7a7a\"])\n",
    "ax.set_title(\"Many to Many Topic, Uniprocessor\")\n",
    "ax.set_ylabel(\"Drop Rate\")\n",
    "ax.set_yscale('log')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "\n",
    "# ax.set_ylim([0, 1])\n",
    "plt.savefig(\"dropped_jobs_many_to_many.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "wcet_grouped_dict = {}\n",
    "for k in wcet_dict.keys():\n",
    "  name = k.split(\"\\n\")[0]\n",
    "  if name not in wcet_grouped_dict.keys():\n",
    "    wcet_grouped_dict[name] = np.array([])\n",
    "  # wcet_dict[k] = wcet_dict[k][wcet_dict[k] < 0.1]\n",
    "  wcet_grouped_dict[name] = np.concatenate((wcet_grouped_dict[name], wcet_dict[k]))\n",
    "sns.violinplot(wcet_grouped_dict)\n",
    "plt.title(\"Many to Many, Uniprocessor\")\n",
    "plt.ylabel(\"WCET (ms)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30,4))\n",
    "ax = sns.violinplot(wcet_dict)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "plt.title(\"Many to Many, Uniprocessor\")\n",
    "plt.ylabel(\"WCET (ms)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TODO: Pie chart of which types of jobs are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 10\n",
    "directory = \"./data/autoware_benchmark/%ds/rmw_cyclonedds_cpp/\" % duration\n",
    "executors = [\"autoware_default_edf\", \"autoware_default_rm\", \"autoware_default_events\", \"autoware_default_singlethreaded\"]\n",
    "dirs = [directory + e for e in executors]\n",
    "\n",
    "files = [directory+'/std_output.log' for directory in dirs]\n",
    "\n",
    "hot_path_name = None\n",
    "\n",
    "# result maps each pair (exe, rmw) to lists of results corresponding to the runs\n",
    "results = defaultdict(lambda: [])\n",
    "\n",
    "hot_path_name_regex = re.compile(r'^ *hot path: *(.*)$')\n",
    "hot_path_latency_regex = re.compile(r'^ *hot path latency: *(.+)ms \\[min=(.+)ms, ' +\n",
    "                                    r'max=(.+)ms, average=(.+)ms, deviation=(.+)ms\\]$')\n",
    "hot_path_drops_regex = re.compile(r'^ *hot path drops: *(.+) \\[min=(.+), max=(.+), ' +\n",
    "                                  r'average=(.+), deviation=(.+)\\]$')\n",
    "behavior_planner_period_regex = re.compile(r'^ *behavior planner period: *(.+)ms \\[' +\n",
    "                                            r'min=(.+)ms, max=(.+)ms, average=(.+)ms, ' +\n",
    "                                            r'deviation=(.+)ms\\]$')\n",
    "\n",
    "rmw_regex = re.compile(r'^RMW Implementation: (rmw_.*)')\n",
    "filename_regex = re.compile(r'.*/([0-9]+)s/(rmw_.*)/(.*)/std_output.log')\n",
    "for count, file in enumerate(files):\n",
    "    match = filename_regex.match(file)\n",
    "    if not match:\n",
    "        raise ValueError(f'File {file} does not conform to the naming scheme')\n",
    "\n",
    "    extracted_duration, rmw, exe = match.groups()\n",
    "    if int(extracted_duration) != duration:\n",
    "        raise ValueError(f'File {file} does not match expected duration {duration}')\n",
    "    with open(file) as fp:\n",
    "        rmw_line, *data = fp.read().splitlines()\n",
    "\n",
    "    match = rmw_regex.match(rmw_line)\n",
    "    if match and rmw != match.groups()[0]:\n",
    "        raise ValueError((f'{file}: mismatch between filename-rmw (\"{rmw}\")' +\n",
    "                          f'and content-rmw(\"{match.groups()[0]}\")'))\n",
    "\n",
    "    if rmw not in file:\n",
    "        raise ValueError(f'File {file} contains data from RMW {rmw}, contradicting its name')\n",
    "\n",
    "    for line in data:\n",
    "        match = hot_path_name_regex.match(line)\n",
    "        if match:\n",
    "            name, = match.groups()\n",
    "            if hot_path_name is not None and hot_path_name != name:\n",
    "                raise ValueError('Two different hotpaths in a single summary: ' +\n",
    "                                  f'{name} {hot_path_name}')\n",
    "            hot_path_name = name\n",
    "            continue\n",
    "        match = hot_path_latency_regex.match(line)\n",
    "        if match:\n",
    "            results[exe].append(float(match.groups()[0]))\n",
    "            continue\n",
    "\n",
    "if hot_path_name is None:\n",
    "    raise RuntimeError('No hot_path defined in experiment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the style of the plot\n",
    "# sns.set_style({'axes.facecolor':'white', 'grid.color': '.8'})\n",
    "# sns.set_context(\"talk\")  # Adjust this for larger or smaller text\n",
    "# results[\"autoware_default_events\"].sort()\n",
    "# # results[\"autoware_default_fifo\"].sort()\n",
    "# results[\"autoware_default_rm\"].sort()\n",
    "# results[\"autoware_default_singlethreaded\"].sort()\n",
    "# results[\"autoware_default_staticsinglethreaded\"].sort()\n",
    "# print(results[\"autoware_default_events\"][-5:])\n",
    "# # print(results[\"autoware_default_fifo\"][-5:])\n",
    "# print(results[\"autoware_default_rm\"][-5:])\n",
    "# print(results[\"autoware_default_singlethreaded\"][-5:])\n",
    "# print(results[\"autoware_default_staticsinglethreaded\"][-5:])\n",
    "\n",
    "# Creating the violin plot with specific color scheme and settings\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "# ax = sns.boxplot(data=results, color=\"#2171b5\", whis=100, linewidth=1.5, linecolor=\"#10385a\",\n",
    "#     fliersize=5, showfliers=False)\n",
    "parts = plt.violinplot([results[\"autoware_default_singlethreaded\"],\n",
    "                     results[\"autoware_default_events\"],\n",
    "                     results[\"autoware_default_rm\"],\n",
    "                     results[\"autoware_default_edf\"]],\n",
    "                     positions=[0,1,2,3], showextrema=True)\n",
    "# for pc in parts['bodies']:\n",
    "#     pc.set_edgecolor('#ff0000')\n",
    "ax = sns.violinplot(data=results, palette=[\"#2171b5\", \"#2171b5\", \"#2171b5\", \"#2171b5\"],\n",
    "                    linewidth=0, inner_kws={\"box_width\": 0, \"whis_width\": 0}, cut=0,\n",
    "                    order=[\"autoware_default_singlethreaded\", \"autoware_default_events\", \"autoware_default_rm\", \"autoware_default_edf\"])\n",
    "# sns.boxplot(results, width=1, whis=100)\n",
    "\n",
    "# Customizing the look and feel of the plot to match the bar graph\n",
    "ax.set_ylabel(\"Latency (ms)\", fontsize=16, labelpad=10)  # Y-axis Label\n",
    "ax.set_title(\"Latency Summary {}s [FrontLidarDriver/RearLidarDriver -> ObjectCollision]\".format(duration), fontsize=16, pad=20)  # Title\n",
    "\n",
    "# Setting y-axis limits and labels similar to the bar chart\n",
    "ax.set_ybound(0, 100)  # Y-axis Bounds\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(10))  # Major ticks every 10 units\n",
    "ax.yaxis.set_minor_locator(ticker.MultipleLocator(2))   # Minor ticks every 2 units\n",
    "\n",
    "# Enable grid only for major ticks on the y-axis\n",
    "ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n",
    "ax.grid(True, which='minor', linestyle='', linewidth=0)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_yticklabels([int(x) for x in ax.get_yticks()], size=12)  # Y-axis Ticks\n",
    "ax.set_xticklabels([\"Default\", \"Events\", \"RM\", \"EDF\"], ha=\"center\", fontsize=16)\n",
    "\n",
    "# Remove top and right borders for a cleaner look\n",
    "sns.despine(fig=None, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"latency_violin.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = \"data/\"\n",
    "\n",
    "samples = {}\n",
    "df = pd.DataFrame(columns=[\"Executor\", \"Utilization\", \"Task\", \"Response Time\"])\n",
    "\n",
    "fifo_executors = [\"Default\", \"Events\"]\n",
    "lifo_executors = [\"RM\", \"EDF\"]\n",
    "utilizations = [\"60%\", \"80%\", \"90%\"]\n",
    "task_df = pd.DataFrame(columns=[\"timestamp\", \"Executor\", \"Utilization\", \"Task\", \"release_time\", \"completion_time\"])\n",
    "\n",
    "\n",
    "for utilization in utilizations:\n",
    "  for executor in fifo_executors:\n",
    "    processor_x_queue = []\n",
    "    processor_y_queue = []\n",
    "    subset_df = callback_df[(callback_df[\"Executor\"] == executor) & (callback_df[\"Utilization\"] == utilization)]\n",
    "    subset_df_sorted = subset_df.sort_values(\"timestamp\")\n",
    "    release_times = {\"SensorA\" : None, \"SensorB\" : None, \"SensorC\" : None}\n",
    "    response_times_dir = \"data/response_time.{}.{}/\".format(executor.lower(), utilization.replace(\"%\", \"\"))\n",
    "    response_times_files = {\"SensorA\" : open(response_times_dir + \"SensorA.node.txt\", 'r'),\n",
    "                            \"SensorB\" : open(response_times_dir + \"SensorB.node.txt\", 'r'),\n",
    "                            \"SensorC\" : open(response_times_dir + \"SensorC.node.txt\", 'r'),\n",
    "                            \"ProcessorX\": open(response_times_dir + \"ProcessorX.node.txt\", 'r'),\n",
    "                            \"ProcessorY\": open(response_times_dir + \"ProcessorY.node.txt\", 'r')}\n",
    "    for index, row in subset_df_sorted.iterrows():\n",
    "      node_name = row[\"Node\"].split(\"\\n\")[0]\n",
    "      if \"Sensor\" in node_name:\n",
    "        file_line = response_times_files[node_name].readline()\n",
    "        sequence_number = int(file_line.split(\":\")[0])\n",
    "        sensor_response_time = timedelta(microseconds=int(file_line.split(\":\")[1]) / 1000.0)\n",
    "        completion_time = row[\"timestamp\"] + row[\"duration\"]\n",
    "        release_time = completion_time - sensor_response_time\n",
    "        \n",
    "        processor_x_queue.append(row[\"timestamp\"])\n",
    "        processor_y_queue.append(row[\"timestamp\"])\n",
    "        \n",
    "        # TODO: Get sequence number from file and save that in the task_df\n",
    "        task_df = pd.concat([task_df, pd.DataFrame({\"timestamp\": [row[\"timestamp\"]],\n",
    "                                                    \"Executor\": [executor],\n",
    "                                                    \"Utilization\": [utilization],\n",
    "                                                    \"Task\": [node_name],\n",
    "                                                    \"release_time\": [release_time],\n",
    "                                                    \"completion_time\": [completion_time],\n",
    "                                                    \"children_executed\": 0,\n",
    "                                                    \"sequence_number\": sequence_number})])\n",
    "        release_times[node_name] = release_time\n",
    "      elif \"Processor\" in node_name:\n",
    "        file_line = response_times_files[node_name].readline()\n",
    "        parent_name = file_line.split(\":\")[0]\n",
    "        sequence_number = int(file_line.split(\":\")[1])\n",
    "        # TODO: Get the sequence number from the processor node file and retrieve the row corresponding to the provided sensor name and sequence number\n",
    "        completion_time = row[\"timestamp\"] + row[\"duration\"]\n",
    "        task_df.loc[(task_df['Task'] == parent_name) & (task_df['sequence_number'] == sequence_number) & (task_df[\"Utilization\"] == utilization) & (task_df[\"Executor\"] == executor), 'completion_time'] = completion_time\n",
    "        task_df.loc[(task_df['Task'] == parent_name) & (task_df['sequence_number'] == sequence_number) & (task_df[\"Utilization\"] == utilization) & (task_df[\"Executor\"] == executor), 'children_executed'] += 1\n",
    "      else:\n",
    "        raise ValueError(\"Unknown node\")\n",
    "      \n",
    "      # TODO: Iterate over events and measure time between each timer and its two children to find E2E latency of each task\n",
    "      \n",
    "  for executor in lifo_executors:\n",
    "    processor_x_queue = []\n",
    "    processor_y_queue = []\n",
    "    subset_df = callback_df[(callback_df[\"Executor\"] == executor) & (callback_df[\"Utilization\"] == utilization)]\n",
    "    subset_df_sorted = subset_df.sort_values(\"timestamp\")\n",
    "    release_times = {\"SensorA\" : None, \"SensorB\" : None, \"SensorC\" : None}\n",
    "    response_times_dir = \"data/response_time.{}.{}/\".format(executor.lower(), utilization.replace(\"%\", \"\"))\n",
    "    response_times_files = {\"SensorA\" : open(response_times_dir + \"SensorA.node.txt\", 'r'),\n",
    "                            \"SensorB\" : open(response_times_dir + \"SensorB.node.txt\", 'r'),\n",
    "                            \"SensorC\" : open(response_times_dir + \"SensorC.node.txt\", 'r')}\n",
    "    for index, row in subset_df_sorted.iterrows():\n",
    "      node_name = row[\"Node\"].split(\"\\n\")[0]\n",
    "      if \"Sensor\" in node_name:\n",
    "        file_line = response_times_files[node_name].readline()\n",
    "        sequence_number = int(file_line.split(\":\")[0])\n",
    "        sensor_response_time = timedelta(microseconds=int(file_line.split(\":\")[1]) / 1000.0)\n",
    "        completion_time = row[\"timestamp\"] + row[\"duration\"]\n",
    "        release_time = completion_time - sensor_response_time\n",
    "        \n",
    "        processor_x_queue.append(row[\"timestamp\"])\n",
    "        processor_y_queue.append(row[\"timestamp\"])\n",
    "        \n",
    "        task_df = pd.concat([task_df, pd.DataFrame({\"timestamp\": [row[\"timestamp\"]],\n",
    "                                                    \"Executor\": [executor],\n",
    "                                                    \"Utilization\": [utilization],\n",
    "                                                    \"Task\": [node_name],\n",
    "                                                    \"release_time\": [release_time],\n",
    "                                                    \"completion_time\": [completion_time],\n",
    "                                                    \"children_executed\": 0,\n",
    "                                                    \"sequence_number\": sequence_number})])\n",
    "        release_times[node_name] = release_time\n",
    "      elif \"Processor\" in node_name:\n",
    "        if \"X\" in node_name:\n",
    "          sensor_timestamp = processor_x_queue.pop()\n",
    "        elif \"Y\" in node_name:\n",
    "          sensor_timestamp = processor_y_queue.pop()\n",
    "        else:\n",
    "          raise ValueError(\"Unknown processor\")\n",
    "        completion_time = row[\"timestamp\"] + row[\"duration\"]\n",
    "        task_df.loc[task_df['timestamp'] == sensor_timestamp, 'completion_time'] = completion_time\n",
    "        task_df.loc[task_df['timestamp'] == sensor_timestamp, 'children_executed'] += 1\n",
    "      else:\n",
    "        raise ValueError(\"Unknown node\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in task_df, compute the difference of release_time and completion_time for each task and save that as response_time\n",
    "\n",
    "\n",
    "task_df[\"Response Time\"] = task_df.apply(lambda row: (row[\"completion_time\"] - row[\"release_time\"]) / timedelta(milliseconds=1), axis=1)\n",
    "\n",
    "task_df[\"Response Time\"] = task_df[\"Response Time\"].astype(float)\n",
    "cutoff = task_df.groupby([\"Task\", \"Executor\", \"Utilization\"])[\"Response Time\"].quantile(0.997)\n",
    "last_timestamps = task_df.groupby([\"Task\", \"Executor\", \"Utilization\"])[\"timestamp\"].max()\n",
    "cutoff_dict = cutoff.to_dict()\n",
    "\n",
    "task_df[\"Cutoff\"] = task_df.apply(lambda row: cutoff_dict[(row[\"Task\"], row[\"Executor\"], row[\"Utilization\"])], axis=1)\n",
    "task_df_filtered = task_df[(task_df[\"Response Time\"] <= task_df[\"Cutoff\"]) & (task_df[\"timestamp\"] < task_df[\"completion_time\"]) & (task_df[\"children_executed\"] == 2)]\n",
    "\n",
    "\n",
    "print(\"SensorA\")\n",
    "print(f\"{'Executor':<15} {'60%':<10} {'80%':<10} {'90%':<10}\")\n",
    "print(f\"{'-'*15} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "print(f\"{'Default':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'Events':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'EDF':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'RM':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print()\n",
    "print(\"SensorB\")\n",
    "print(f\"{'Executor':<15} {'60%':<10} {'80%':<10} {'90%':<10}\")\n",
    "print(f\"{'-'*15} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "print(f\"{'Default':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorB') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'Events':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorB') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'EDF':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorB') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'RM':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorB') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print()\n",
    "print(\"SensorC\")\n",
    "print(f\"{'Executor':<15} {'60%':<10} {'80%':<10} {'90%':<10}\")\n",
    "print(f\"{'-'*15} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "print(f\"{'Default':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorC') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Default') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'Events':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorC') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'Events') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'EDF':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorC') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'EDF') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "print(f\"{'RM':<15} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorC') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '60%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '80%')]['Response Time']):.3f} {'':<3} {max(task_df_filtered[(task_df_filtered['Task'] == 'SensorA') & (task_df_filtered['Executor'] == 'RM') & (task_df_filtered['Utilization'] == '90%')]['Response Time']):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.violinplot(data=task_df_filtered[task_df_filtered[\"Task\"] == \"SensorA\"], x=\"Executor\", y=\"Response Time\", hue=\"Utilization\",\n",
    "    palette=[\"#004d4c\", \"#008080\", \"#5ca3a3\"], linewidth=0.5, cut=0, hue_order=[\"60%\", \"80%\", \"90%\"],\n",
    "    order=[\"Default\", \"Events\", \"EDF\", \"RM\"],\n",
    "    inner_kws={\"box_width\": 1, \"whis_width\": 0})\n",
    "ax.set_title(\"Response Time, SensorA\")\n",
    "ax.set_ylabel(\"Response Time (ms)\")\n",
    "ax.set_ylim([0, 30])\n",
    "ax.set_xticklabels([\"Default\", \"Events\", \"EDF\", \"RM\"], ha=\"center\", rotation=30)\n",
    "\n",
    "plt.savefig(\"response_times_sensora.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.violinplot(data=task_df_filtered[task_df_filtered[\"Task\"] == \"SensorB\"], x=\"Executor\", y=\"Response Time\", hue=\"Utilization\",\n",
    "    palette=[\"#004d4c\", \"#008080\", \"#5ca3a3\"], linewidth=0.5, cut=0, hue_order=[\"60%\", \"80%\", \"90%\"],\n",
    "    order=[\"Default\", \"Events\", \"EDF\", \"RM\"],\n",
    "    inner_kws={\"box_width\": 1, \"whis_width\": 0})\n",
    "ax.set_title(\"Response Time, SensorB\")\n",
    "ax.set_ylabel(\"Response Time (ms)\")\n",
    "ax.set_ylim([0, 50])\n",
    "ax.set_xticklabels([\"Default\", \"Events\", \"EDF\", \"RM\"], ha=\"center\", rotation=30)\n",
    "\n",
    "plt.savefig(\"response_times_sensorb.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.violinplot(data=task_df_filtered[task_df_filtered[\"Task\"] == \"SensorA\"], x=\"Executor\", y=\"Response Time\", hue=\"Utilization\",\n",
    "    palette=[\"#004d4c\", \"#008080\", \"#5ca3a3\"], linewidth=0.5, cut=0, hue_order=[\"60%\", \"80%\", \"90%\"],\n",
    "    order=[\"Default\", \"Events\", \"EDF\", \"RM\"],\n",
    "    inner_kws={\"box_width\": 1, \"whis_width\": 0})\n",
    "ax.set_title(\"Response Time, SensorC\")\n",
    "ax.set_ylabel(\"Response Time (ms)\")\n",
    "ax.set_ylim([0, 60])\n",
    "ax.set_xticklabels([\"Default\", \"Events\", \"EDF\", \"RM\"], ha=\"center\", rotation=30)\n",
    "\n",
    "plt.savefig(\"response_times_sensorc.svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
